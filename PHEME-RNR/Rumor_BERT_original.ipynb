{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec3077e",
   "metadata": {},
   "source": [
    "### Step 1.1 : Data Viewing and Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f01a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.data.processors.utils import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc778bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_comments</th>\n",
       "      <th>text_only</th>\n",
       "      <th>comments_only</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>Breaking: At least 10 dead, 5 injured after tO...</td>\n",
       "      <td>The religion of peace strikes again.\\n[SEP]Hi ...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>France: 10 people dead after shooting at HQ of...</td>\n",
       "      <td>MT France: 10 dead after shooting at HQ of sat...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten killed in shooting at headquarters of Fren...</td>\n",
       "      <td>Ten killed in shooting at headquarters of Fren...</td>\n",
       "      <td>must be that peace loving religion again\\n[SEP...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: 10 dead in shooting at headquarters ...</td>\n",
       "      <td>BREAKING: 10 dead in shooting at headquarters ...</td>\n",
       "      <td>WTF &amp;gt; BREAKING 10 dead in shooting at headq...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reuters: 10 people shot dead at headquarters o...</td>\n",
       "      <td>Reuters: 10 people shot dead at headquarters o...</td>\n",
       "      <td>watch yourself in Paris bud\\n[SEP]islamist ter...</td>\n",
       "      <td>rumour</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_comments  \\\n",
       "0  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "1  France: 10 people dead after shooting at HQ of...   \n",
       "2  Ten killed in shooting at headquarters of Fren...   \n",
       "3  BREAKING: 10 dead in shooting at headquarters ...   \n",
       "4  Reuters: 10 people shot dead at headquarters o...   \n",
       "\n",
       "                                           text_only  \\\n",
       "0  Breaking: At least 10 dead, 5 injured after tO...   \n",
       "1  France: 10 people dead after shooting at HQ of...   \n",
       "2  Ten killed in shooting at headquarters of Fren...   \n",
       "3  BREAKING: 10 dead in shooting at headquarters ...   \n",
       "4  Reuters: 10 people shot dead at headquarters o...   \n",
       "\n",
       "                                       comments_only   label  count  \n",
       "0  The religion of peace strikes again.\\n[SEP]Hi ...  rumour      9  \n",
       "1  MT France: 10 dead after shooting at HQ of sat...  rumour      7  \n",
       "2  must be that peace loving religion again\\n[SEP...  rumour      5  \n",
       "3  WTF &gt; BREAKING 10 dead in shooting at headq...  rumour     13  \n",
       "4  watch yourself in Paris bud\\n[SEP]islamist ter...  rumour     16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data/raw_data.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f731331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes are used for data statistics only. No need to uncomment.\n",
    "# data = raw_data[raw_data['count'] > 0]\n",
    "# print(data['count'].min())\n",
    "# print(data['count'].mean())\n",
    "\n",
    "# raw_data['len_text'] =raw_data.text_comments.apply(lambda x: len(x.split()))\n",
    "\n",
    "# print(raw_data['len_text'].median())\n",
    "# bins = [0,50,100,150,200,250,300,350,400,450,500]\n",
    "# groups = pd.cut(raw_data['len_text'],bins,right=True)\n",
    "# pd.value_counts(groups).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f39d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_comments</th>\n",
       "      <th>text_only</th>\n",
       "      <th>comments_only</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>The black &amp;amp; unarmed group on the left is c...</td>\n",
       "      <td>The black &amp;amp; unarmed group on the left is c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>→ Charlie Hebdo attack: Hunt for killers focus...</td>\n",
       "      <td>→ Charlie Hebdo attack: Hunt for killers focus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>Plane crashes in southern France, 148 on board...</td>\n",
       "      <td>Plane crashes in southern France, 148 on board...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Terrorists shoot officer in Paris during terro...</td>\n",
       "      <td>Terrorists shoot officer in Paris during terro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>It says much that men with guns feel quite so ...</td>\n",
       "      <td>It says much that men with guns feel quite so ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>NEWS Germanwings flight crashes in south of Fr...</td>\n",
       "      <td>NEWS Germanwings flight crashes in south of Fr...</td>\n",
       "      <td>is this true\\n[SEP]</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>Just landed, Barcelona to Kaunas. Found out ab...</td>\n",
       "      <td>Just landed, Barcelona to Kaunas. Found out ab...</td>\n",
       "      <td>BREAKING - Germanwings plane crashes in France...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>#BREAKING: A helicopter has located the Airbus...</td>\n",
       "      <td>#BREAKING: A helicopter has located the Airbus...</td>\n",
       "      <td>RT  #BREAKING: A helicopter has located the Ai...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>.@Maryam_Rajavi message regarding the terroris...</td>\n",
       "      <td>.@Maryam_Rajavi message regarding the terroris...</td>\n",
       "      <td>Cartoons drawn for Charlie Hebdo:  #CharlieHeb...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>#Germanwings Airbus 320 was on its way from Ba...</td>\n",
       "      <td>#Germanwings Airbus 320 was on its way from Ba...</td>\n",
       "      <td>“@CBSThisMorning: #Germanwings Airbus 320 was ...</td>\n",
       "      <td>nonrumour</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_comments  \\\n",
       "2484  The black &amp; unarmed group on the left is c...   \n",
       "260   → Charlie Hebdo attack: Hunt for killers focus...   \n",
       "3325  Plane crashes in southern France, 148 on board...   \n",
       "729   Terrorists shoot officer in Paris during terro...   \n",
       "728   It says much that men with guns feel quite so ...   \n",
       "...                                                 ...   \n",
       "3506  NEWS Germanwings flight crashes in south of Fr...   \n",
       "3511  Just landed, Barcelona to Kaunas. Found out ab...   \n",
       "3518  #BREAKING: A helicopter has located the Airbus...   \n",
       "1253  .@Maryam_Rajavi message regarding the terroris...   \n",
       "3525  #Germanwings Airbus 320 was on its way from Ba...   \n",
       "\n",
       "                                              text_only  \\\n",
       "2484  The black &amp; unarmed group on the left is c...   \n",
       "260   → Charlie Hebdo attack: Hunt for killers focus...   \n",
       "3325  Plane crashes in southern France, 148 on board...   \n",
       "729   Terrorists shoot officer in Paris during terro...   \n",
       "728   It says much that men with guns feel quite so ...   \n",
       "...                                                 ...   \n",
       "3506  NEWS Germanwings flight crashes in south of Fr...   \n",
       "3511  Just landed, Barcelona to Kaunas. Found out ab...   \n",
       "3518  #BREAKING: A helicopter has located the Airbus...   \n",
       "1253  .@Maryam_Rajavi message regarding the terroris...   \n",
       "3525  #Germanwings Airbus 320 was on its way from Ba...   \n",
       "\n",
       "                                          comments_only      label  count  \n",
       "2484                                                NaN  nonrumour      0  \n",
       "260                                                 NaN     rumour      0  \n",
       "3325                                                NaN     rumour      0  \n",
       "729                                                 NaN  nonrumour      0  \n",
       "728                                                 NaN  nonrumour      0  \n",
       "...                                                 ...        ...    ...  \n",
       "3506                                is this true\\n[SEP]  nonrumour      1  \n",
       "3511  BREAKING - Germanwings plane crashes in France...  nonrumour      1  \n",
       "3518  RT  #BREAKING: A helicopter has located the Ai...  nonrumour      1  \n",
       "1253  Cartoons drawn for Charlie Hebdo:  #CharlieHeb...  nonrumour      1  \n",
       "3525  “@CBSThisMorning: #Germanwings Airbus 320 was ...  nonrumour      1  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.sort_values(by='count', inplace=True)\n",
    "raw_data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac565718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0358b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT ###\n",
    "# You may change 'model_path' to save and load different trained models.\n",
    "# Availiable options: 'text_comments','text_only','commments_only','comments_group1','comments_group2','comments_group3','natural_split','fixed_split'.\n",
    "# Please make sure that your 'model_path' must match the correspongding data and comments.\n",
    "# For more details, please check the 'README.md' file.\n",
    "\n",
    "model_path = 'text_comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5ff3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different Number of Comments ##\n",
    "\n",
    "# Please uncomment the corresponding lines if the 'model_path' is 'comments_groupX'.\n",
    "\n",
    "# print(raw_data['count'].describe(percentiles=[0.33,0.67]))\n",
    "\n",
    "# For'comments_group1'.\n",
    "# raw_data = raw_data[raw_data['count'] <= 7]\n",
    "# raw_data.shape\n",
    "\n",
    "# For'comments_group2'.\n",
    "# raw_data = raw_data[raw_data['count'] > 7]\n",
    "# raw_data = raw_data[raw_data['count'] <= 18]\n",
    "# raw_data.shape\n",
    "\n",
    "# For'comments_group3'.\n",
    "# raw_data = raw_data[raw_data['count'] > 18]\n",
    "# raw_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48190230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>The black &amp;amp; unarmed group on the left is c...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>→ Charlie Hebdo attack: Hunt for killers focus...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>Plane crashes in southern France, 148 on board...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Terrorists shoot officer in Paris during terro...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>It says much that men with guns feel quite so ...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      label\n",
       "2484  The black &amp; unarmed group on the left is c...  nonrumour\n",
       "260   → Charlie Hebdo attack: Hunt for killers focus...     rumour\n",
       "3325  Plane crashes in southern France, 148 on board...     rumour\n",
       "729   Terrorists shoot officer in Paris during terro...  nonrumour\n",
       "728   It says much that men with guns feel quite so ...  nonrumour"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Selection ##\n",
    "\n",
    "# You may change 'text_comments' to 'text_only' or 'comments_only' with the corresponding 'model_path' to get more experiment results.\n",
    "\n",
    "raw_data = raw_data[['text_comments','label']]\n",
    "raw_data = raw_data.rename(columns = {'text_comments':'text'})\n",
    "\n",
    "# raw_data = raw_data[['text_only','label']]\n",
    "# raw_data = raw_data.rename(columns = {'text_only':'text'})\n",
    "\n",
    "# raw_data = raw_data[['comments_only','label']]\n",
    "# raw_data = raw_data.rename(columns = {'comments_only':'text'})\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c168aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5802, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = raw_data.dropna(axis=0)\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43adc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>The black &amp;amp; unarmed group on the left is c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>→ Charlie Hebdo attack: Hunt for killers focus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>Plane crashes in southern France, 148 on board...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Terrorists shoot officer in Paris during terro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>It says much that men with guns feel quite so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "2484  The black &amp; unarmed group on the left is c...      0\n",
       "260   → Charlie Hebdo attack: Hunt for killers focus...      1\n",
       "3325  Plane crashes in southern France, 148 on board...      1\n",
       "729   Terrorists shoot officer in Paris during terro...      0\n",
       "728   It says much that men with guns feel quite so ...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['label'] = LabelEncoder().fit_transform(raw_data['label'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e1f495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>Reports: Crashed #Germanwings plane was carryi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Charlie Hebdo became well known for publishing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>Ferguson, Mo., is 67% black, but black citizen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>The live stream from #Ferguson is back up and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>The new world we live in #CharlieHebdo\\n[SEP]I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4693</th>\n",
       "      <td>The PM's office releases a statement about  #s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5792</th>\n",
       "      <td>A hostage situation at a Sydney cafe has come ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>At 87 years of age, Asterix cartoonist Uderzo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>Updates on #CharlieHebdo shooting from staff &amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>Map shows Martin Place in Sydney where hostage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "3297  Reports: Crashed #Germanwings plane was carryi...      1\n",
       "458   Charlie Hebdo became well known for publishing...      0\n",
       "2811  Ferguson, Mo., is 67% black, but black citizen...      0\n",
       "2879  The live stream from #Ferguson is back up and ...      0\n",
       "1878  The new world we live in #CharlieHebdo\\n[SEP]I...      0\n",
       "4693  The PM's office releases a statement about  #s...      1\n",
       "5792  A hostage situation at a Sydney cafe has come ...      0\n",
       "1772  At 87 years of age, Asterix cartoonist Uderzo ...      0\n",
       "1078  Updates on #CharlieHebdo shooting from staff &...      0\n",
       "4735  Map shows Martin Place in Sydney where hostage...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = raw_data.copy()\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15a36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.2, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d317016d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All passengers and crew feared dead after A320...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING - Germanwings #A320 from Barcelona to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>→ 41 Charlie Hebdo Paris shooting: New killing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Update - AFP reports at least two people kille...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let´s get serious about #CharlieHebdo and West...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This made me tear up, I am SO proud to be Aust...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Incidents occurred at National War Memorial, n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Police: 2 hostage situations near Paris believ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In sympathy and solidarity with  #CharlieHebdo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Salman Rushdie: “Respect for religion” has bec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  All passengers and crew feared dead after A320...      1\n",
       "1  BREAKING - Germanwings #A320 from Barcelona to...      1\n",
       "2  → 41 Charlie Hebdo Paris shooting: New killing...      0\n",
       "3  Update - AFP reports at least two people kille...      1\n",
       "4  Let´s get serious about #CharlieHebdo and West...      0\n",
       "5  This made me tear up, I am SO proud to be Aust...      0\n",
       "6  Incidents occurred at National War Memorial, n...      1\n",
       "7  Police: 2 hostage situations near Paris believ...      0\n",
       "8  In sympathy and solidarity with  #CharlieHebdo...      0\n",
       "9  Salman Rushdie: “Respect for religion” has bec...      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6155b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4641, 2), (1161, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3a980",
   "metadata": {},
   "source": [
    "### Step 1.2 : Split the Dataset into Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f8ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_split,get_natural_split,get_fixed_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7880649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All passengers and crew feared dead after A320...</td>\n",
       "      <td>1</td>\n",
       "      <td>[All passengers and crew feared dead after A32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING - Germanwings #A320 from Barcelona to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[BREAKING - Germanwings #A320 from Barcelona t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>→ 41 Charlie Hebdo Paris shooting: New killing...</td>\n",
       "      <td>0</td>\n",
       "      <td>[→ 41 Charlie Hebdo Paris shooting: New killin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Update - AFP reports at least two people kille...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Update - AFP reports at least two people kill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let´s get serious about #CharlieHebdo and West...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Let´s get serious about #CharlieHebdo and Wes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  All passengers and crew feared dead after A320...      1   \n",
       "1  BREAKING - Germanwings #A320 from Barcelona to...      1   \n",
       "2  → 41 Charlie Hebdo Paris shooting: New killing...      0   \n",
       "3  Update - AFP reports at least two people kille...      1   \n",
       "4  Let´s get serious about #CharlieHebdo and West...      0   \n",
       "\n",
       "                                          text_split  \n",
       "0  [All passengers and crew feared dead after A32...  \n",
       "1  [BREAKING - Germanwings #A320 from Barcelona t...  \n",
       "2  [→ 41 Charlie Hebdo Paris shooting: New killin...  \n",
       "3  [Update - AFP reports at least two people kill...  \n",
       "4  [Let´s get serious about #CharlieHebdo and Wes...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tmp = train.copy()\n",
    "\n",
    "### IMPORTANT ###\n",
    "# If your 'model_path' is 'natural_split' please use 'get_natural_split' function.\n",
    "# If your 'model_path' is 'fixed_split' please use 'get_fixed_split' function.\n",
    "\n",
    "train_tmp['text_split'] = train['text'].apply(get_split)\n",
    "# train_tmp['text_split'] = train['text'].apply(get_fixed_split)\n",
    "# train_tmp['text_split'] = train['text'].apply(get_natural_split)\n",
    "train = train_tmp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc58a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grand Mufti Prof. Ibrahim Abu Mohamed has cond...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Grand Mufti Prof. Ibrahim Abu Mohamed has con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before his death #MikeBrown told his mother, \"...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Before his death #MikeBrown told his mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The reason to have a #JeNeSuisPasCharlie convo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[The reason to have a #JeNeSuisPasCharlie conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charlie Hebdo: There is no way they will make ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Charlie Hebdo: There is no way they will make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French police still haven’t found the #Charlie...</td>\n",
       "      <td>0</td>\n",
       "      <td>[French police still haven’t found the #Charli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Grand Mufti Prof. Ibrahim Abu Mohamed has cond...      0   \n",
       "1  Before his death #MikeBrown told his mother, \"...      1   \n",
       "2  The reason to have a #JeNeSuisPasCharlie convo...      0   \n",
       "3  Charlie Hebdo: There is no way they will make ...      0   \n",
       "4  French police still haven’t found the #Charlie...      0   \n",
       "\n",
       "                                          text_split  \n",
       "0  [Grand Mufti Prof. Ibrahim Abu Mohamed has con...  \n",
       "1  [Before his death #MikeBrown told his mother, ...  \n",
       "2  [The reason to have a #JeNeSuisPasCharlie conv...  \n",
       "3  [Charlie Hebdo: There is no way they will make...  \n",
       "4  [French police still haven’t found the #Charli...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tmp = val.copy()\n",
    "\n",
    "### IMPORTANT ###\n",
    "# If your 'model_path' is 'natural_split' please use 'get_natural_split' function.\n",
    "# If your 'model_path' is 'fixed_split' please use 'get_fixed_split' function.\n",
    "\n",
    "val_tmp['text_split'] = val['text'].apply(get_split)\n",
    "# val_tmp['text_split'] = val['text'].apply(get_fixed_split)\n",
    "# val_tmp['text_split'] = val['text'].apply(get_natural_split)\n",
    "val = val_tmp\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "461b2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6791, 6791, 6791)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l = []  # Segmented Text\n",
    "label_l = []  # Label of Each Text\n",
    "index_l =[]   # The Index of Each Text Before Segmentation\n",
    "for idx,row in train.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    train_l.append(l)\n",
    "    label_l.append(row['label'])\n",
    "    index_l.append(idx)\n",
    "len(train_l), len(label_l), len(index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca1239b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1746, 1746, 1746)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for idx,row in val.iterrows():\n",
    "  for l in row['text_split']:\n",
    "    val_l.append(l)\n",
    "    val_label_l.append(row['label'])\n",
    "    val_index_l.append(idx)\n",
    "len(val_l), len(val_label_l), len(val_index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e06d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All passengers and crew feared dead after A320...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING - Germanwings #A320 from Barcelona to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>→ 41 Charlie Hebdo Paris shooting: New killing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Update - AFP reports at least two people kille...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let´s get serious about #CharlieHebdo and West...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  All passengers and crew feared dead after A320...      1\n",
       "1  BREAKING - Germanwings #A320 from Barcelona to...      1\n",
       "2  → 41 Charlie Hebdo Paris shooting: New killing...      0\n",
       "3  Update - AFP reports at least two people kille...      1\n",
       "4  Let´s get serious about #CharlieHebdo and West...      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame({'text':train_l, 'label':label_l})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08e92fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grand Mufti Prof. Ibrahim Abu Mohamed has cond...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before his death #MikeBrown told his mother, \"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The reason to have a #JeNeSuisPasCharlie convo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charlie Hebdo: There is no way they will make ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French police still haven’t found the #Charlie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Grand Mufti Prof. Ibrahim Abu Mohamed has cond...      0\n",
       "1  Before his death #MikeBrown told his mother, \"...      1\n",
       "2  The reason to have a #JeNeSuisPasCharlie convo...      0\n",
       "3  Charlie Hebdo: There is no way they will make ...      0\n",
       "4  French police still haven’t found the #Charlie...      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.DataFrame({'text':val_l, 'label':val_label_l})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b9eb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train_df.apply(lambda x: InputExample(guid=None,text_a = x['text'], text_b = None, label = x['label']), axis = 1)\n",
    "\n",
    "val_InputExamples = val_df.apply(lambda x: InputExample(guid=None, text_a = x['text'], text_b = None, label = x['label']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c94ce1",
   "metadata": {},
   "source": [
    "### Step 2 : Define Models For Bert Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fdadd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    BertTokenizer,\n",
    "    BertweetTokenizer,\n",
    "    AutoModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "from transformers.data.processors.utils import InputExample, DataProcessor\n",
    "\n",
    "import logging\n",
    "\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94f5144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES={\n",
    "    \"bert\":(BertConfig,BertTokenizer),\n",
    "    \"bertweet\":(BertConfig,BertweetTokenizer)\n",
    "}\n",
    "\n",
    "my_label_list=[0, 1]\n",
    "MAX_SEQ_LENGTH=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70daba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 2\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output=outputs[:2]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        \n",
    "        outputs = (logits, pooled_output, sequence_output,)\n",
    "\n",
    "        if labels is not None:\n",
    "            \n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        \n",
    "        return outputs  # loss, logits, pooled_output, sequence_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b72192",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 3.1 : Load Pre-training Models & Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb4e239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Pre-training Models\n",
    "# args={\"model_name_or_path\": \"bert-base-uncased\",\n",
    "#     \"config_name\": \"bert-base-uncased\",\n",
    "#     \"tokenizer_name\": \"bert-base-uncased\",\n",
    "#       }\n",
    "\n",
    "# config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "# model_class=BertForClassification\n",
    "\n",
    "\n",
    "# config = config_class.from_pretrained(\n",
    "#     args[\"config_name\"],\n",
    "#     finetuning_task=\"\", \n",
    "#     cache_dir=None,\n",
    "# )\n",
    "# tokenizer = tokenizer_class.from_pretrained(\n",
    "#     args[\"tokenizer_name\"],\n",
    "#     do_lower_case=True,\n",
    "#     cache_dir=None,\n",
    "# )\n",
    "# model = model_class.from_pretrained(\n",
    "#     args[\"model_name_or_path\"],\n",
    "#     from_tf=bool(\".ckpt\" in args[\"model_name_or_path\"]),\n",
    "#     config=config,\n",
    "#     cache_dir=None,\n",
    "# )\n",
    "\n",
    "\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b19bfa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare Training Data\n",
    "# train_features = convert_examples_to_features(train_InputExamples,tokenizer, label_list=my_label_list, \n",
    "#                                               output_mode=\"classification\", max_length=MAX_SEQ_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "081134e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "# attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "# token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "# the_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "# dataset = TensorDataset(input_ids, attention_mask, token_type_ids, the_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89481e18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 3.2 : Train & Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d094b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Train Function For Bert Classification\n",
    "\n",
    "# def train(train_dataset,model,tokenizer):\n",
    "#     no_decay=[\"bias\",\"LayerNorm.weight\"]\n",
    "#     optimizer_grouped_parameters=[\n",
    "#         {\n",
    "#             \"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#             \"weight_decay\":0.0,\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             \"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#             \"weight_decay\":0.0\n",
    "#         },\n",
    "#     ]\n",
    "\n",
    "    \n",
    "#     t_total=len(train_dataset)// 5\n",
    "#     optimizer=AdamW(optimizer_grouped_parameters,lr=2e-5,eps=1e-8)\n",
    "    \n",
    "#     scheduler=get_linear_schedule_with_warmup(\n",
    "#         optimizer,num_warmup_steps=0,num_training_steps=t_total\n",
    "#         )\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # *********************\n",
    "#     logger.info(\"*****Running training*****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "#     logger.info(\"  Num Epochs = %d\", 5)\n",
    "\n",
    "\n",
    "#     epochs_trained=0\n",
    "#     global_step=0\n",
    "#     steps_trained_in_current_epoch=0\n",
    "\n",
    "#     tr_loss,logging_loss=0.0,0.0\n",
    "#     model.zero_grad()\n",
    "#     train_iterator=trange(epochs_trained,5,desc=\"Epoch\",disable=False)\n",
    "\n",
    "\n",
    "#     for k in train_iterator: #5 epoch\n",
    "    \n",
    "#         train_sampler=RandomSampler(train_dataset)\n",
    "#         train_dataloader=DataLoader(train_dataset,sampler=train_sampler,batch_size=16)\n",
    "#         epoch_iterator=tqdm(train_dataloader,desc=\"Iteration\",disable=False)\n",
    "\n",
    "#         for step,batch in enumerate(epoch_iterator): \n",
    "#             if steps_trained_in_current_epoch>0:\n",
    "#                 steps_trained_in_current_epoch-=1\n",
    "#                 continue\n",
    "\n",
    "#             model.train()\n",
    "#             batch=tuple(t.to(\"cuda\") for t in batch)\n",
    "            \n",
    "#             inputs={\"input_ids\": batch[0],\"attention_mask\": batch[1],\"token_type_ids\": batch[2], \"labels\": batch[3]}\n",
    "#             outputs = model(**inputs)\n",
    "#             loss=outputs[0]\n",
    " \n",
    "#             loss.backward()\n",
    "\n",
    "#             tr_loss+=loss.item()\n",
    "#             if (step+1)%1==0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 model.zero_grad()\n",
    "#                 global_step+=1\n",
    "\n",
    "#         logger.info(\"average loss:\" +str(tr_loss/global_step))\n",
    "\n",
    "\n",
    "#     return global_step,tr_loss/global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be86380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start Training\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# train(dataset,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1ed6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Trained Model Parameters\n",
    "\n",
    "# import os\n",
    "# model.save_pretrained(\"./trained_models/classification_models_\" + model_path)\n",
    "# tokenizer.save_pretrained(\"./trained_models/classification_models_\" + model_path)\n",
    "\n",
    "# torch.save(args,os.path.join(\"./trained_models/classification_models_\" + model_path,\"training_args.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3204b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 4.1 : Load the Trained Model & Prepare Data for Bert Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10539792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Loading the trained model data\n",
    "\n",
    "args_eval={\"model_name_or_path\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"config_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"tokenizer_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "      }\n",
    "\n",
    "config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "model_class=BertForClassification\n",
    "\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args_eval[\"config_name\"],\n",
    "    finetuning_task=\"\", \n",
    "    cache_dir=None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args_eval[\"tokenizer_name\"],\n",
    "    do_lower_case=True,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args_eval[\"model_name_or_path\"],\n",
    "    from_tf=bool(\".ckpt\" in args_eval[\"model_name_or_path\"]),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2a5770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\R_BERT\\lib\\site-packages\\transformers\\data\\processors\\glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data for Evaluation\n",
    "\n",
    "val_features = convert_examples_to_features(val_InputExamples, tokenizer, label_list=my_label_list, output_mode=\"classification\",  max_length=MAX_SEQ_LENGTH )\n",
    "\n",
    "\n",
    "val_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
    "val_attention_mask = torch.tensor([f.attention_mask for f in val_features], dtype=torch.long)\n",
    "val_token_type_ids = torch.tensor([f.token_type_ids for f in val_features], dtype=torch.long)\n",
    "val_the_labels = torch.tensor([f.label for f in val_features], dtype=torch.long)\n",
    "\n",
    "\n",
    "eval_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_the_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27663f",
   "metadata": {},
   "source": [
    "### Step 4.2 : Bert Classification Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e422409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a5ba125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, eval_dataset):\n",
    "\n",
    "\n",
    "    logger.info(\"***** Running evaluation  *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", 16)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    eval_sampler =RandomSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=16)\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    accuracy,f1 = acc_and_f1(preds, out_label_ids)\n",
    "\n",
    "\n",
    "    return accuracy,f1,eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7b2f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37e7256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 110/110 [00:11<00:00,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9379624359704041 F1 Score:  0.8995391705069123 Loss:  0.1821067643859847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy,f1 ,eval_loss = evaluate(model, tokenizer, eval_dataset)\n",
    "\n",
    "print(\"Accuracy: \",accuracy, \"F1 Score: \",f1,\"Loss: \",eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cc642",
   "metadata": {},
   "source": [
    "### Step 5.1 : Get Text Embeddings & Combine Embeddings with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "309a5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, tokenizer, dataset):\n",
    "\n",
    "    logger.info(\"***** Running prediction  *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", 16)\n",
    "\n",
    "    pooled_outputs = None\n",
    "\n",
    "    sampler =SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=32)\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(\"cpu\") for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            pooled_output = outputs[2]\n",
    "\n",
    "            if pooled_outputs is None:\n",
    "                pooled_outputs = pooled_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                pooled_outputs = np.append(pooled_outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    return pooled_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb0f8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_eval={\"model_name_or_path\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"config_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "    \"tokenizer_name\": \"./trained_models/classification_models_\" + model_path,\n",
    "      }\n",
    "\n",
    "\n",
    "config_class, tokenizer_class = MODEL_CLASSES[\"bert\"]\n",
    "model_class=BertForClassification\n",
    "\n",
    "\n",
    "config = config_class.from_pretrained(\n",
    "    args_eval[\"config_name\"],\n",
    "    finetuning_task=\"\", \n",
    "    cache_dir=None,\n",
    ")\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    args_eval[\"tokenizer_name\"],\n",
    "    do_lower_case=True,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model = model_class.from_pretrained(\n",
    "    args_eval[\"model_name_or_path\"],\n",
    "    from_tf=bool(\".ckpt\" in args_eval[\"model_name_or_path\"]),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "\n",
    "\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b830b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(train_InputExamples,tokenizer, label_list=my_label_list, output_mode=\"classification\", max_length=MAX_SEQ_LENGTH )\n",
    "\n",
    "val_features = convert_examples_to_features(val_InputExamples, tokenizer, label_list=my_label_list, output_mode=\"classification\",  max_length=MAX_SEQ_LENGTH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "999a86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "train_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
    "train_token_type_ids = torch.tensor([f.token_type_ids for f in train_features], dtype=torch.long)\n",
    "train_the_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_token_type_ids, train_the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76e5ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_input_ids = torch.tensor([f.input_ids for f in val_features], dtype=torch.long)\n",
    "val_attention_mask = torch.tensor([f.attention_mask for f in val_features], dtype=torch.long)\n",
    "val_token_type_ids = torch.tensor([f.token_type_ids for f in val_features], dtype=torch.long)\n",
    "val_the_labels = torch.tensor([f.label for f in val_features], dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_the_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "297a56c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████| 213/213 [39:37<00:00, 11.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6791, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pooled_outputs = get_prediction(model, tokenizer, train_dataset)\n",
    "train_pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4d3df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 55/55 [09:24<00:00, 10.26s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1746, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pooled_outputs = get_prediction(model, tokenizer, val_dataset)\n",
    "val_pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a729bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.59535825, 0.22133827, -0.064959064, 0.286...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.03500444, 0.48017377, -0.0060315444, -0.2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.60248107, -0.36487758, -0.023010079, 0.122...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.54029393, 0.24938576, 0.5865337, -0.04925...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.53922117, -0.45679554, 0.12911734, 0.17952...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.630705, -0.21441744, 0.8533651, -0.0937718...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[-0.5761065, 0.23694296, 0.040113527, -0.0188...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[0.09709701, -0.20442064, -0.72128963, 0.2803...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.48888493, -0.36267853, 0.6068989, 0.070144...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[0.28556317, -0.47153112, 0.69443136, 0.27363...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[-0.59535825, 0.22133827, -0.064959064, 0.286...      1\n",
       "1  [[-0.03500444, 0.48017377, -0.0060315444, -0.2...      1\n",
       "2  [[0.60248107, -0.36487758, -0.023010079, 0.122...      0\n",
       "3  [[-0.54029393, 0.24938576, 0.5865337, -0.04925...      1\n",
       "4  [[0.53922117, -0.45679554, 0.12911734, 0.17952...      0\n",
       "5  [[0.630705, -0.21441744, 0.8533651, -0.0937718...      0\n",
       "6  [[-0.5761065, 0.23694296, 0.040113527, -0.0188...      1\n",
       "7  [[0.09709701, -0.20442064, -0.72128963, 0.2803...      0\n",
       "8  [[0.48888493, -0.36267853, 0.6068989, 0.070144...      0\n",
       "9  [[0.28556317, -0.47153112, 0.69443136, 0.27363...      0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Feature Concatenation\n",
    "train_x = {}\n",
    "# print(index_l)\n",
    "for l, emb in zip(index_l, train_pooled_outputs):\n",
    "    # print(l)\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c931f6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.68049216, -0.26034823, -0.4906886, -0.0074...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.92846686, 0.501028, 0.941066, -0.9366698, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.4172276, -0.3754966, 0.62748855, 0.3172978...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.55138415, -0.105106525, 0.6977908, -0.1001...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.8112874, -0.14650372, 0.7057784, 0.272934...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.64057475, -0.3374844, 0.06960024, -0.01647...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.5044643, -0.30644172, -0.19273518, 0.30636...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.33219296, 0.0666637, -0.3166317, -0.09060...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[-0.51913524, -0.39834502, 0.31883827, 0.6338...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[0.538575, -0.42830735, -0.1163139, 0.1630735...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.68049216, -0.26034823, -0.4906886, -0.0074...      0\n",
       "1  [[0.92846686, 0.501028, 0.941066, -0.9366698, ...      1\n",
       "2  [[0.4172276, -0.3754966, 0.62748855, 0.3172978...      0\n",
       "3  [[0.55138415, -0.105106525, 0.6977908, -0.1001...      0\n",
       "4  [[-0.8112874, -0.14650372, 0.7057784, 0.272934...      0\n",
       "5  [[0.64057475, -0.3374844, 0.06960024, -0.01647...      0\n",
       "6  [[0.5044643, -0.30644172, -0.19273518, 0.30636...      0\n",
       "7  [[-0.33219296, 0.0666637, -0.3166317, -0.09060...      1\n",
       "8  [[-0.51913524, -0.39834502, 0.31883827, 0.6338...      0\n",
       "9  [[0.538575, -0.42830735, -0.1163139, 0.1630735...      0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Feature Concatenation\n",
    "val_x = {}\n",
    "\n",
    "for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "    if l in val_x.keys():\n",
    "        val_x[l]  =np.vstack([val_x[l], emb])\n",
    "    else:\n",
    "        val_x[l] = [emb]\n",
    "\n",
    "\n",
    "val_l_final = []\n",
    "vlabel_l_final = []\n",
    "for k in val_x.keys():\n",
    "    val_l_final.append(val_x[k])\n",
    "    vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a84d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2: Feature Average Pooling\n",
    "# train_x = {}\n",
    "# for l, emb in zip(index_l, train_pooled_outputs):\n",
    "#     if l in train_x.keys():\n",
    "#         train_x[l]  =np.vstack([train_x[l], emb])\n",
    "#     else:\n",
    "#         train_x[l] = [emb]\n",
    "\n",
    "# for l in train_x.keys():\n",
    "#     # print(len(train_x[l]))\n",
    "#     train_x[l] = [np.mean(train_x[l],axis=0)]\n",
    "\n",
    "# train_l_final = []\n",
    "# label_l_final = []\n",
    "# for k in train_x.keys():\n",
    "#     train_l_final.append(train_x[k])\n",
    "#     label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "# df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee847f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 2: Feature Average Pooling\n",
    "# val_x = {}\n",
    "\n",
    "# for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "#     if l in val_x.keys():\n",
    "#         val_x[l]  =np.vstack([val_x[l], emb])\n",
    "#     else:\n",
    "#         val_x[l] = [emb]\n",
    "\n",
    "# for l in val_x.keys():\n",
    "#     val_x[l] = [np.mean(val_x[l],axis=0)]\n",
    "\n",
    "# val_l_final = []\n",
    "# vlabel_l_final = []\n",
    "# for k in val_x.keys():\n",
    "#     val_l_final.append(val_x[k])\n",
    "#     vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "# df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "# df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52ad73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 3: Feature Max Pooling\n",
    "# train_x = {}\n",
    "# for l, emb in zip(index_l, train_pooled_outputs):\n",
    "#     if l in train_x.keys():\n",
    "#         train_x[l]  =np.vstack([train_x[l], emb])\n",
    "#     else:\n",
    "#         train_x[l] = [emb]\n",
    "\n",
    "# for l in train_x.keys():\n",
    "#     # print(len(train_x[l]))\n",
    "#     train_x[l] = [np.max(train_x[l],axis=0)]\n",
    "\n",
    "# train_l_final = []\n",
    "# label_l_final = []\n",
    "# for k in train_x.keys():\n",
    "#     train_l_final.append(train_x[k])\n",
    "#     label_l_final.append(train.loc[k]['label'])\n",
    "\n",
    "# df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f8fa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 3: Feature Max Pooling\n",
    "# val_x = {}\n",
    "\n",
    "# for l, emb in zip(val_index_l, val_pooled_outputs):\n",
    "#     if l in val_x.keys():\n",
    "#         val_x[l]  =np.vstack([val_x[l], emb])\n",
    "#     else:\n",
    "#         val_x[l] = [emb]\n",
    "\n",
    "# for l in val_x.keys():\n",
    "#     val_x[l] = [np.max(val_x[l],axis=0)]\n",
    "\n",
    "# val_l_final = []\n",
    "# vlabel_l_final = []\n",
    "# for k in val_x.keys():\n",
    "#     val_l_final.append(val_x[k])\n",
    "#     vlabel_l_final.append(val.loc[k]['label'])\n",
    "\n",
    "# df_val = pd.DataFrame({'emb': val_l_final, 'label': vlabel_l_final})\n",
    "# df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "882b784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(df_val, test_size=0.4, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4faadf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4641, 2), (696, 2), (465, 2))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff7d01",
   "metadata": {},
   "source": [
    "### Step 5.2 : Prepare Data for Classfication Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "204ebd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "    'text_comments':[[7,663],[3,232],[5,93]],\n",
    "    'text_only':[[7,663],[3,232],[5,93]],\n",
    "    'comments_only':[[4,1088],[4,163],[4,109]],\n",
    "    'comments_group1':[[4,387],[4,58],[5,31]],\n",
    "    'comments_group2':[[4,398],[1,239],[4,40]],\n",
    "    'comments_group3':[[5,300],[5,45],[1,151]],\n",
    "    'natural_split':[[7,663],[3,232],[5,93]],\n",
    "    'fixed_split':[[7,663],[3,232],[5,93]],\n",
    "}\n",
    "\n",
    "batches = batch_dict[model_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a8941fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size = batches[0][0], batches_per_epoch = batches[0][1]):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "\n",
    "def val_generator(df,batch_size_val=batches[1][0],batches_per_epoch_val=batches[1][1]):\n",
    "    \n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= 768\n",
    "\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "\n",
    "def test_generator(df,batch_size_test=batches[2][0],batches_per_epoch_test=batches[2][1]):\n",
    "    \n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= 768\n",
    "\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1af6ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train)\n",
    "val_data = val_generator(df_val)\n",
    "test_data = test_generator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c6fcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label)),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred)),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee93622",
   "metadata": {},
   "source": [
    "### Step 6.1 : Train & Save LSTM Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d2b3594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 13:52:51.508202: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 13:52:51.523212: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399410000 Hz\n",
      "2025-04-29 13:52:51.523935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x30b8c2f0 executing computations on platform Host. Devices:\n",
      "2025-04-29 13:52:51.523949: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "2025-04-29 13:52:51.524404: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 350,692\n",
      "Trainable params: 350,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "# keras.layers.Masking(mask_value=0.0)\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "\n",
    "# Which we encoded in a single vector via a LSTM\n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "# And we add a softmax classifier on top\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "# At model instantiation, we specify the input and the output:\n",
    "model = keras.Model(text_input, out)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c3ae957",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "888919aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 663 steps, validate for 232 steps\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x77818fee2cb0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x77818fee2cb0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 13:53:44.087397: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_5844_7301' and '__inference___backward_standard_lstm_7406_8003_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8125' both implement 'lstm_3319ed09-f65e-4aa3-b0c6-5b81e84d45df' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659/663 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 13:53:55.856698: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_10344_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_12215' and '__inference_standard_lstm_10344' both implement 'lstm_72f074a0-c42f-4b78-97e4-1f2074838059' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663/663 [==============================] - 15s 23ms/step - loss: 0.1451 - acc: 0.9532 - val_loss: 0.1518 - val_acc: 0.9454\n",
      "Epoch 2/10\n",
      "663/663 [==============================] - 12s 18ms/step - loss: 0.1275 - acc: 0.9571 - val_loss: 0.1499 - val_acc: 0.9454\n",
      "Epoch 3/10\n",
      "663/663 [==============================] - 11s 16ms/step - loss: 0.1223 - acc: 0.9604 - val_loss: 0.1420 - val_acc: 0.9540\n",
      "Epoch 4/10\n",
      "658/663 [============================>.] - ETA: 0s - loss: 0.1174 - acc: 0.9603\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "663/663 [==============================] - 12s 19ms/step - loss: 0.1172 - acc: 0.9604 - val_loss: 0.1481 - val_acc: 0.9483\n",
      "Epoch 5/10\n",
      "663/663 [==============================] - 11s 17ms/step - loss: 0.1125 - acc: 0.9651 - val_loss: 0.1412 - val_acc: 0.9497\n",
      "Epoch 6/10\n",
      "663/663 [==============================] - 11s 16ms/step - loss: 0.1070 - acc: 0.9662 - val_loss: 0.1388 - val_acc: 0.9526\n",
      "Epoch 7/10\n",
      "660/663 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9654\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "663/663 [==============================] - 12s 18ms/step - loss: 0.1025 - acc: 0.9655 - val_loss: 0.1453 - val_acc: 0.9468\n",
      "Epoch 8/10\n",
      "663/663 [==============================] - 12s 18ms/step - loss: 0.0962 - acc: 0.9685 - val_loss: 0.1496 - val_acc: 0.9440\n",
      "Epoch 9/10\n",
      "663/663 [==============================] - 12s 18ms/step - loss: 0.0903 - acc: 0.9696 - val_loss: 0.1649 - val_acc: 0.9353\n",
      "Epoch 10/10\n",
      "658/663 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9705\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "663/663 [==============================] - 12s 18ms/step - loss: 0.0860 - acc: 0.9707 - val_loss: 0.1610 - val_acc: 0.9339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x77818d6f34d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batches[0][1]\n",
    "\n",
    "batches_per_epoch_val= batches[1][1]\n",
    "\n",
    "model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "716252c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./trained_models/classification_models_\" + model_path + \"/LSTM_model/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eec162db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d5e97",
   "metadata": {},
   "source": [
    "### Step 6.2 : Evaluate LSTM Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be396c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12842/2662434083.py\", line 1, in <module>\n",
      "    model = tf.keras.models.load_model(save_path)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\n",
      "    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 166, in load_model_from_hdf5\n",
      "    model_config = json.loads(model_config.decode('utf-8'))\n",
      "AttributeError: 'str' object has no attribute 'decode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12842/2662434083.py\", line 1, in <module>\n",
      "    model = tf.keras.models.load_model(save_path)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\n",
      "    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 166, in load_model_from_hdf5\n",
      "    model_config = json.loads(model_config.decode('utf-8'))\n",
      "AttributeError: 'str' object has no attribute 'decode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12842/2662434083.py\", line 1, in <module>\n",
      "    model = tf.keras.models.load_model(save_path)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\n",
      "    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 166, in load_model_from_hdf5\n",
      "    model_config = json.loads(model_config.decode('utf-8'))\n",
      "AttributeError: 'str' object has no attribute 'decode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3396, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/home/angelos/anaconda3/envs/py_env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82954709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
     ]
    }
   ],
   "source": [
    "batches_per_epoch_test = batches[2][1]\n",
    "pred = model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c482027e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.95699,\n",
       " 'precision': 0.91667,\n",
       " 'recall': 0.95333,\n",
       " 'f1-score': 0.93464}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(pred,axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528db35",
   "metadata": {},
   "source": [
    "### Step 7.1 : Train & Save Transformer Model For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9ed39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75094044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bd3c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b8632e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=768\n",
    "ff_dim=32\n",
    "num_heads=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67d54eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransformerLayer.call of <__main__.TransformerLayer object at 0x778196391a10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TransformerLayer.call of <__main__.TransformerLayer object at 0x778196391a10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method TransformerLayer.call of <__main__.TransformerLayer object at 0x778196391a10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING:tensorflow:Entity <bound method MultiHeadSelfAttention.call of <__main__.MultiHeadSelfAttention object at 0x778158080390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiHeadSelfAttention.call of <__main__.MultiHeadSelfAttention object at 0x778158080390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiHeadSelfAttention.call of <__main__.MultiHeadSelfAttention object at 0x778158080390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 768)         2415392   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 2,766,084\n",
      "Trainable params: 2,766,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='text')\n",
    "\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "\n",
    "encoded_text = TransformerLayer(embed_dim,num_heads,ff_dim)(l_mask)\n",
    "\n",
    "out_dense1 = keras.layers.LSTM(100,)(encoded_text)\n",
    "\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(out_dense1)\n",
    "\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "\n",
    "model = keras.Model(text_input, out)\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f6761bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9fe100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 663 steps, validate for 232 steps\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x77814e132a70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x77814e132a70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x77814e132a70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 14:01:41.414374: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_83790_84273_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_84851' and '__inference___backward_cudnn_lstm_with_fallback_83503_83685' both implement 'lstm_c9e2550a-f1de-44f7-a0d3-b7f4f7b9b9ab' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662/663 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 14:02:06.767111: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_87415' and '__inference_standard_lstm_87304_specialized_for_model_1_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_87670' both implement 'lstm_6eb6a6ab-648e-48af-a766-7cdc22a22e88' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663/663 [==============================] - 30s 45ms/step - loss: 0.1648 - acc: 0.9498 - val_loss: 0.1372 - val_acc: 0.9569\n",
      "Epoch 2/10\n",
      "663/663 [==============================] - 24s 37ms/step - loss: 0.1458 - acc: 0.9565 - val_loss: 0.1624 - val_acc: 0.9382\n",
      "Epoch 3/10\n",
      "663/663 [==============================] - 24s 36ms/step - loss: 0.1436 - acc: 0.9550 - val_loss: 0.1557 - val_acc: 0.9468\n",
      "Epoch 4/10\n",
      "661/663 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9583\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "663/663 [==============================] - 25s 38ms/step - loss: 0.1357 - acc: 0.9582 - val_loss: 0.1490 - val_acc: 0.9454\n",
      "Epoch 5/10\n",
      "663/663 [==============================] - 26s 39ms/step - loss: 0.1348 - acc: 0.9573 - val_loss: 0.1629 - val_acc: 0.9440\n",
      "Epoch 6/10\n",
      "663/663 [==============================] - 25s 38ms/step - loss: 0.1379 - acc: 0.9560 - val_loss: 0.1623 - val_acc: 0.9397\n",
      "Epoch 7/10\n",
      "661/663 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9531\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "663/663 [==============================] - 24s 37ms/step - loss: 0.1458 - acc: 0.9530 - val_loss: 0.1492 - val_acc: 0.9454\n",
      "Epoch 8/10\n",
      "663/663 [==============================] - 26s 39ms/step - loss: 0.1375 - acc: 0.9571 - val_loss: 0.1595 - val_acc: 0.9325\n",
      "Epoch 9/10\n",
      "663/663 [==============================] - 26s 38ms/step - loss: 0.1321 - acc: 0.9573 - val_loss: 0.1451 - val_acc: 0.9454\n",
      "Epoch 10/10\n",
      "662/663 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9581\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "663/663 [==============================] - 27s 40ms/step - loss: 0.1351 - acc: 0.9580 - val_loss: 0.1538 - val_acc: 0.9454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x77814e1076d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batches[0][1]\n",
    "\n",
    "batches_per_epoch_val= batches[1][1]\n",
    "\n",
    "model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96fb0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_weight_path = \"./trained_models/classification_models_\" + model_path + \"/Transformer_model/model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a45dddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(save_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf0c48",
   "metadata": {},
   "source": [
    "### Step 7.2 : Evaluate Transformer Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e398c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_generator(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7598acdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights(save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42935439",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batches[2][1]\n",
    "\n",
    "pred = model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a9f36d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96344,\n",
       " 'precision': 0.93464,\n",
       " 'recall': 0.95333,\n",
       " 'f1-score': 0.94389}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(pred,axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae6935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyEnv",
   "language": "python",
   "name": "my_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
